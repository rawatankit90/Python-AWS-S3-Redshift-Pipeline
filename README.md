# Sparkify Song Data Analysis on AWS Cloud

## Problem Statement

Sparkify is an online new music streaming app. They have collected user activity info
and want to do analysis on what songs the users are listening to by doing analytical
search on the data.Sparkify, has grown their user base and song database and want to
move their processes and data onto the cloud. Their data resides in S3, in a directory
of JSON logs on user activity on the app, as well as a directory with JSON metadata on
the songs in their app

The query that the user wants to run

## Solution

The solution that is designed to solve the above problem is to build an ETL pipeline that
extracts their data from S3, stages them in Redshift, and transforms data into a set of
dimensional tables for their analytics team to continue finding insights in what songs
their users are listening to

## Software

*  AWS
    Redshift
    S3
    IAM

*  Python 3.6 with pandaand boto3 library
*  Jupyter Notebooks

## Programming languages

*  SQL
*  Python
*  JSON
*  AWS boto3

## DataSet

*  Song Dataset
    The first dataset is a subset of real data from the Million Song Dataset. Each file is in
    JSON format and contains metadata about a song and the artist of that song.

*  Log Dataset
    The  dataset consists of log files in JSON format generated by this event simulator based on
    the songs in the dataset above. These simulate app activity logs from a music streaming app
    based on specified  configurations.

## Files in Repo

1. `Create Cluster.ipynb` : Jupyter Notebook to setup/delete the RedShift cluster
2. `dwh.cfg`              : Config files to store AWS KEYS, Dataset locations and cluster properties
3. `sql_queries.py`       : File contains all the queries used to create table and copy data from S3 into those TABLES
4. `create_tables.py`     : File Contains the functions to use the queries present in `sql_queries.py` to create tables
                            in RedShift Cluster
5. `etl.py`               : File Contains the functions to use the queries present in `sql_queries.py` to populate the tables
                            created earlier in RedShift Cluster

## How to run :point_down:

Download the files and then follow below steps

1. Populate KEY and SECRET in dwh.cfg section of your AWS account with Admin access
2. Run 'Create Cluster.ipynb' Jupyter notebook uptill step 9 to create AWS Redshift cluster
3. Run create_tables.py to create the tables
4. Run etl.py to populate the tables with S3 Data
5. Validate the tables `staging_events`, `staging_songs` for data inserted from
    JSON files present
6. Validate the tables `songplays`, `users`, `songs`, `artists`, `time` for data inserted from
    staging tables
7. Run Step 12 onwards in 'Create Cluster.ipynb' Jupyter notebook  to free the resources created in AWS.
